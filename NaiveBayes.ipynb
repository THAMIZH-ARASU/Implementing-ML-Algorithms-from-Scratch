{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes Overview**\n",
    "\n",
    "Naive Bayes is a statistical and machine learning technique commonly used for classification problems, especially in cases involving text or categorical data. It is based on Bayes' Theorem and assumes that the features are conditionally independent given the class labelâ€”a simplification known as the \"naive\" assumption. Naive Bayes calculates the probability of a data point belonging to a particular class based on the prior probabilities and the likelihood of the observed features. Despite its simplicity, it often performs surprisingly well, especially in high-dimensional spaces.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "### \\\\[ P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)} \\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## In our case\n",
    "\n",
    "### \\\\[ P(y \\mid X) = \\frac{P(X \\mid y) \\cdot P(y)}{P(X)} \\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with feature vector X\n",
    "\n",
    "### \\\\[ \\mathbf{X} = (x_1, x_2, x_3, \\ldots, x_n) \\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assume that all features are mutually independent\n",
    "\n",
    "### \\\\[ P(y \\mid \\mathbf{X}) = \\frac{P(x_1 \\mid y) \\cdot P(x_2 \\mid y) \\cdots P(x_n \\mid y) \\cdot P(y)}{P(\\mathbf{X})} \\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the class with highest probability\n",
    "\n",
    "#### \\\\[ y = \\arg\\max_y P(y \\mid \\mathbf{X}) = \\arg\\max_y \\frac{P(x_1 \\mid y) \\cdot P(x_2 \\mid y) \\cdots P(x_n \\mid y) \\cdot P(y)}{P(\\mathbf{X})} \\\\]\n",
    "\n",
    "#### \\\\[ y = \\arg\\max_y P(x_1 \\mid y) \\cdot P(x_2 \\mid y) \\cdots P(x_n \\mid y) \\cdot P(y) \\\\]\n",
    "\n",
    "#### \\\\[ y = \\arg\\max_y \\log(P(x_1 \\mid y)) + \\log(P(x_2 \\mid y)) + \\cdots + \\log(P(x_n \\mid y)) + \\log(P(y)) \\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior probability P(y): frequency\n",
    "\n",
    "## Class conditional probability P(x_i|y)\n",
    "\n",
    "### \\\\[ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\cdot \\exp\\left( -\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2} \\right) \\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype = np.float64)\n",
    "        self._variance = np.zeros((n_classes, n_features), dtype = np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype = np.float64)\n",
    "\n",
    "        for cls in self._classes:\n",
    "            X_cls = X[cls == y]\n",
    "            self._mean[cls, :] = X_cls.mean(axis = 0)\n",
    "            self._variance[cls, :] = X_cls.var(axis = 0)\n",
    "            self._priors[cls] = X_cls.shape[0] / float(n_samples)\n",
    "\n",
    "    def _pdf(self, class_index, X):\n",
    "        mean = self._mean[class_index]\n",
    "        variance = self._variance[class_index]\n",
    "        numerator = np.exp(- (X - mean) ** 2 / (2 * variance))\n",
    "        denominator = np.sqrt(2 * np.pi * variance)\n",
    "\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        posteriors = []\n",
    "\n",
    "        for index, cls in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[index])\n",
    "            class_conditional = np.sum(np.log(self._pdf(index, X)))\n",
    "            posterior = prior + class_conditional\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return y_pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we'll test our implementation of of the Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples = 1000, n_features = 10, n_classes = 2, random_state = 98)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting using the implemented Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.915\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_model = NaiveBayes()\n",
    "naive_bayes_model.fit(X_train, y_train)\n",
    "predictions = naive_bayes_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the Simple Naive Bayes is implemented in python and executed succussfully. You can try with different random_states fo the dataset to see how it reflect on the accuracy of our model..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
